{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Automated Machine Learning\n",
    "This is the code for the paper entitled \"**[IoT Data Analytics in Dynamic Environments: From An Automated Machine Learning Perspective](https://arxiv.org/abs/2209.08018)**\" published in *Engineering Applications of Artificial Intelligence* (Elsevier's Journal, IF:7.8).<br>\n",
    "Authors: Li Yang (lyang339@uwo.ca) and Abdallah Shami (Abdallah.Shami@uwo.ca)<br>\n",
    "Organization: The Optimized Computing and Communications (OC2) Lab, ECE Department, Western University\n",
    "\n",
    "L. Yang and A. Shami, \"IoT Data Analytics in Dynamic Environments: From An Automated Machine Learning Perspective\", *Engineering Applications of Artificial Intelligence*, vol. 116, pp. 1-33, 2022, doi: https://doi.org/10.1016/j.engappai.2022.105366."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Part 1: Automated Offline/Static/Batch Learning\n",
    "Batch learning: Batch learning methods analyze static data in batches and often need access to the entire dataset prior to model training. Traditional ML algorithms can effectively solve batch learning tasks. Although batch learning models often achieve high performance due to their ability to learn diverse data patterns, it is often difficult to update these models once created. Therefore, batch learning faces two significant challenges: model degradation and data unavailability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2: IoTID20\n",
    "A subset of the IoT network traffic data randomly sampled from the [IoTID20 dataset](https://sites.google.com/view/iot-network-intrusion-dataset/home).   \n",
    "\n",
    "IoTID20 dataset was created by using normal and attack virtual machines as network platforms, simulating IoT services with the node-red tool, and extracting features with the Information Security Center of Excellence (ISCX) flow meter program. A typical smart home environment was established for generating this dataset using five IoT devices or services: a smart fridge, a smart thermostat, motion-activated lights, a weather station, and a remotely-activated garage door. Thus, the traffic data samples of normal and abnormal IoT devices are collected in Pcap files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import shapiro\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the sampled IoTID20 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/IoT_2020_b_0.01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flow_ID</th>\n",
       "      <th>Src_IP</th>\n",
       "      <th>Src_Port</th>\n",
       "      <th>Dst_IP</th>\n",
       "      <th>Dst_Port</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow_Duration</th>\n",
       "      <th>TotLen_Fwd_Pkts</th>\n",
       "      <th>TotLen_Bwd_Pkts</th>\n",
       "      <th>Bwd_Pkt_Len_Max</th>\n",
       "      <th>...</th>\n",
       "      <th>Fwd_Pkts/s</th>\n",
       "      <th>Bwd_Pkts/s</th>\n",
       "      <th>Pkt_Len_Std</th>\n",
       "      <th>FIN_Flag_Cnt</th>\n",
       "      <th>Pkt_Size_Avg</th>\n",
       "      <th>Init_Bwd_Win_Byts</th>\n",
       "      <th>Idle_Mean</th>\n",
       "      <th>Idle_Max</th>\n",
       "      <th>Idle_Min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12708</td>\n",
       "      <td>25886</td>\n",
       "      <td>53190</td>\n",
       "      <td>200</td>\n",
       "      <td>9020</td>\n",
       "      <td>3760</td>\n",
       "      <td>124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2776.0</td>\n",
       "      <td>1388.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16129.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2082.0</td>\n",
       "      <td>1869</td>\n",
       "      <td>124.00</td>\n",
       "      <td>124.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>25883</td>\n",
       "      <td>60357</td>\n",
       "      <td>110</td>\n",
       "      <td>7760</td>\n",
       "      <td>3649</td>\n",
       "      <td>46189</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.30</td>\n",
       "      <td>43.30</td>\n",
       "      <td>698.97</td>\n",
       "      <td>0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>126</td>\n",
       "      <td>15396.33</td>\n",
       "      <td>40225.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12695</td>\n",
       "      <td>25883</td>\n",
       "      <td>9020</td>\n",
       "      <td>203</td>\n",
       "      <td>52739</td>\n",
       "      <td>2261</td>\n",
       "      <td>73</td>\n",
       "      <td>1388.0</td>\n",
       "      <td>1388.0</td>\n",
       "      <td>1388.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13698.63</td>\n",
       "      <td>13698.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2082.0</td>\n",
       "      <td>1869</td>\n",
       "      <td>73.00</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12690</td>\n",
       "      <td>25886</td>\n",
       "      <td>52717</td>\n",
       "      <td>200</td>\n",
       "      <td>9020</td>\n",
       "      <td>1963</td>\n",
       "      <td>149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13422.82</td>\n",
       "      <td>6711.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32679</td>\n",
       "      <td>74.50</td>\n",
       "      <td>75.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12690</td>\n",
       "      <td>25886</td>\n",
       "      <td>52717</td>\n",
       "      <td>200</td>\n",
       "      <td>9020</td>\n",
       "      <td>1905</td>\n",
       "      <td>83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12048.19</td>\n",
       "      <td>12048.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32422</td>\n",
       "      <td>83.00</td>\n",
       "      <td>83.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6247</th>\n",
       "      <td>12659</td>\n",
       "      <td>25886</td>\n",
       "      <td>10125</td>\n",
       "      <td>200</td>\n",
       "      <td>9020</td>\n",
       "      <td>1057</td>\n",
       "      <td>70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2776.0</td>\n",
       "      <td>1388.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28571.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2082.0</td>\n",
       "      <td>1869</td>\n",
       "      <td>70.00</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6248</th>\n",
       "      <td>13708</td>\n",
       "      <td>25883</td>\n",
       "      <td>60158</td>\n",
       "      <td>233</td>\n",
       "      <td>8899</td>\n",
       "      <td>3264</td>\n",
       "      <td>40</td>\n",
       "      <td>128.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100000.00</td>\n",
       "      <td>25000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>38.4</td>\n",
       "      <td>-1</td>\n",
       "      <td>10.00</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6249</th>\n",
       "      <td>7655</td>\n",
       "      <td>15614</td>\n",
       "      <td>8487</td>\n",
       "      <td>200</td>\n",
       "      <td>554</td>\n",
       "      <td>3697</td>\n",
       "      <td>1283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1558.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14600</td>\n",
       "      <td>1283.00</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6250</th>\n",
       "      <td>62696</td>\n",
       "      <td>25889</td>\n",
       "      <td>64783</td>\n",
       "      <td>233</td>\n",
       "      <td>9988</td>\n",
       "      <td>3192</td>\n",
       "      <td>222</td>\n",
       "      <td>96.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13513.51</td>\n",
       "      <td>4504.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>74.00</td>\n",
       "      <td>206.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6251</th>\n",
       "      <td>47156</td>\n",
       "      <td>7</td>\n",
       "      <td>443</td>\n",
       "      <td>205</td>\n",
       "      <td>43238</td>\n",
       "      <td>3299</td>\n",
       "      <td>184</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5434.78</td>\n",
       "      <td>5434.78</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2161.5</td>\n",
       "      <td>252</td>\n",
       "      <td>184.00</td>\n",
       "      <td>184.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6252 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Flow_ID  Src_IP  Src_Port  Dst_IP  Dst_Port  Timestamp  Flow_Duration  \\\n",
       "0       12708   25886     53190     200      9020       3760            124   \n",
       "1          60   25883     60357     110      7760       3649          46189   \n",
       "2       12695   25883      9020     203     52739       2261             73   \n",
       "3       12690   25886     52717     200      9020       1963            149   \n",
       "4       12690   25886     52717     200      9020       1905             83   \n",
       "...       ...     ...       ...     ...       ...        ...            ...   \n",
       "6247    12659   25886     10125     200      9020       1057             70   \n",
       "6248    13708   25883     60158     233      8899       3264             40   \n",
       "6249     7655   15614      8487     200       554       3697           1283   \n",
       "6250    62696   25889     64783     233      9988       3192            222   \n",
       "6251    47156       7       443     205     43238       3299            184   \n",
       "\n",
       "      TotLen_Fwd_Pkts  TotLen_Bwd_Pkts  Bwd_Pkt_Len_Max  ...  Fwd_Pkts/s  \\\n",
       "0                 0.0           2776.0           1388.0  ...        0.00   \n",
       "1              2528.0              0.0              0.0  ...       43.30   \n",
       "2              1388.0           1388.0           1388.0  ...    13698.63   \n",
       "3                 0.0              0.0              0.0  ...    13422.82   \n",
       "4                 0.0              0.0              0.0  ...    12048.19   \n",
       "...               ...              ...              ...  ...         ...   \n",
       "6247              0.0           2776.0           1388.0  ...        0.00   \n",
       "6248            128.0             32.0             32.0  ...   100000.00   \n",
       "6249              0.0              0.0              0.0  ...        0.00   \n",
       "6250             96.0             32.0             32.0  ...    13513.51   \n",
       "6251           1441.0           1441.0           1441.0  ...     5434.78   \n",
       "\n",
       "      Bwd_Pkts/s  Pkt_Len_Std  FIN_Flag_Cnt  Pkt_Size_Avg  Init_Bwd_Win_Byts  \\\n",
       "0       16129.03         0.00             0        2082.0               1869   \n",
       "1          43.30       698.97             0         632.0                126   \n",
       "2       13698.63         0.00             0        2082.0               1869   \n",
       "3        6711.41         0.00             0           0.0              32679   \n",
       "4       12048.19         0.00             0           0.0              32422   \n",
       "...          ...          ...           ...           ...                ...   \n",
       "6247    28571.43         0.00             0        2082.0               1869   \n",
       "6248    25000.00         0.00             0          38.4                 -1   \n",
       "6249     1558.85         0.00             0           0.0              14600   \n",
       "6250     4504.50         0.00             0          40.0                 -1   \n",
       "6251     5434.78         0.00             0        2161.5                252   \n",
       "\n",
       "      Idle_Mean  Idle_Max  Idle_Min  Label  \n",
       "0        124.00     124.0     124.0      1  \n",
       "1      15396.33   40225.0    1910.0      1  \n",
       "2         73.00      73.0      73.0      1  \n",
       "3         74.50      75.0      74.0      1  \n",
       "4         83.00      83.0      83.0      1  \n",
       "...         ...       ...       ...    ...  \n",
       "6247      70.00      70.0      70.0      1  \n",
       "6248      10.00      21.0       5.0      1  \n",
       "6249    1283.00    1283.0    1283.0      1  \n",
       "6250      74.00     206.0       8.0      1  \n",
       "6251     184.00     184.0     184.0      1  \n",
       "\n",
       "[6252 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Automated Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Transformation/Encoding\n",
    "Automatically identify and transform string/text features into numerical features to make the data more readable by ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the automated data encoding function\n",
    "def Auto_Encoding(df):\n",
    "    cat_features=[x for x in df.columns if df[x].dtype==\"object\"] ## Find string/text features\n",
    "    le=LabelEncoder()\n",
    "    for col in cat_features:\n",
    "        if col in df.columns:\n",
    "            i = df.columns.get_loc(col)\n",
    "            # Transform to numerical features\n",
    "            df.iloc[:,i] = df.apply(lambda i:le.fit_transform(i.astype(str)), axis=0, result_type='expand')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=Auto_Encoding(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Imputation\n",
    "Detect and impute missing values to improve data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the automated data imputation function\n",
    "def Auto_Imputation(df):\n",
    "    if df.isnull().values.any() or np.isinf(df).values.any(): # if there is any empty or infinite values\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.fillna(0, inplace = True)  # Replace empty values with zeros; there are other imputation methods discussed in the paper\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=Auto_Imputation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated normalization\n",
    "Normalize the range of features to a similar scale to improve data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Auto_Normalization(df):\n",
    "    stat, p = shapiro(df)\n",
    "    print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "    # interpret\n",
    "    alpha = 0.05\n",
    "    numeric_features = df.drop(['Label'],axis = 1).dtypes[df.dtypes != 'object'].index\n",
    "    \n",
    "    # The selection strategy is based on the following article: \n",
    "    # https://medium.com/@kumarvaishnav17/standardization-vs-normalization-in-machine-learning-3e132a19c8bf\n",
    "    # Check if the data distribution follows a Gaussian/normal distribution\n",
    "    # If so, select the Z-score normalization method; otherwise, select the min-max normalization\n",
    "    # Details are in the paper\n",
    "    if p > alpha:\n",
    "        print('Sample looks Gaussian (fail to reject H0)')\n",
    "        df[numeric_features] = df[numeric_features].apply(\n",
    "            lambda x: (x - x.mean()) / (x.std()))\n",
    "        print('Z-score normalization is automatically chosen and used')\n",
    "    else:\n",
    "        print('Sample does not look Gaussian (reject H0)')\n",
    "        df[numeric_features] = df[numeric_features].apply(\n",
    "            lambda x: (x - x.min()) / (x.max()-x.min()))\n",
    "        print('Min-max normalization is automatically chosen and used')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics=0.108, p=0.000\n",
      "Sample does not look Gaussian (reject H0)\n",
      "Min-max normalization is automatically chosen and used\n"
     ]
    }
   ],
   "source": [
    "df=Auto_Normalization(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train-test split\n",
    "Split the dataset into the training and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Label'],axis=1)\n",
    "y = df['Label']\n",
    "\n",
    "# Here we used the 80%/20% split, it can be changed based on specific tasks\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2, shuffle=False,random_state = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated data balancing\n",
    "Generate minority class samples to solve class-imbalance and improve data quality.  \n",
    "Synthetic Minority Over-sampling Technique (SMOTE) method is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4717\n",
       "0     284\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For binary data (can be modified for multi-class data with the same logic)\n",
    "def Auto_Balancing(X_train, y_train):\n",
    "    number0 = pd.Series(y_train).value_counts().iloc[0]\n",
    "    number1 = pd.Series(y_train).value_counts().iloc[1]\n",
    "    \n",
    "    if number0 > number1:\n",
    "        nlarge = number0\n",
    "    else:\n",
    "        nlarge = number1\n",
    "    \n",
    "    # evaluate whether the incoming dataset is imbalanced (the abnormal/normal ratio is smaller than a threshold (e.g., 50%)) \n",
    "    if (number1/number0 > 1.5) or (number0/number1 > 1.5):\n",
    "        smote=SMOTE(n_jobs=-1,sampling_strategy={0:nlarge, 1:nlarge})\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = Auto_Balancing(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4717\n",
       "0    4717\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model learning (for Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0%\n",
      "Precision: 100.0%\n",
      "Recall: 100.0%\n",
      "F1-score: 100.0%\n",
      "Time: 4.78267\n",
      "Wall time: 245 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lg = lgb.LGBMClassifier(verbose = -1)\n",
    "lg.fit(X_train,y_train)\n",
    "t1=time.time()\n",
    "predictions = lg.predict(X_test)\n",
    "t2=time.time()\n",
    "print(\"Accuracy: \"+str(round(accuracy_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Precision: \"+str(round(precision_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Recall: \"+str(round(recall_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"F1-score: \"+str(round(f1_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Time: \"+str(round((t2-t1)/len(y_test)*1000000,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.83999999999999%\n",
      "Precision: 99.83%\n",
      "Recall: 100.0%\n",
      "F1-score: 99.91499999999999%\n",
      "Time: 12.75663\n",
      "Wall time: 915 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "t1=time.time()\n",
    "predictions = rf.predict(X_test)\n",
    "t2=time.time()\n",
    "print(\"Accuracy: \"+str(round(accuracy_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Precision: \"+str(round(precision_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Recall: \"+str(round(recall_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"F1-score: \"+str(round(f1_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Time: \"+str(round((t2-t1)/len(y_test)*1000000,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.584%\n",
      "Precision: 99.876%\n",
      "Recall: 68.73899999999999%\n",
      "F1-score: 81.43299999999999%\n",
      "Time: 1.56773\n",
      "Wall time: 25.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "t1=time.time()\n",
    "predictions = nb.predict(X_test)\n",
    "t2=time.time()\n",
    "print(\"Accuracy: \"+str(round(accuracy_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Precision: \"+str(round(precision_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Recall: \"+str(round(recall_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"F1-score: \"+str(round(f1_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Time: \"+str(round((t2-t1)/len(y_test)*1000000,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.801%\n",
      "Precision: 99.82799999999999%\n",
      "Recall: 98.893%\n",
      "F1-score: 99.358%\n",
      "Time: 220.03672\n",
      "Wall time: 281 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train,y_train)\n",
    "t1=time.time()\n",
    "predictions = knn.predict(X_test)\n",
    "t2=time.time()\n",
    "print(\"Accuracy: \"+str(round(accuracy_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Precision: \"+str(round(precision_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Recall: \"+str(round(recall_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"F1-score: \"+str(round(f1_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Time: \"+str(round((t2-t1)/len(y_test)*1000000,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input,Dense,Dropout,BatchNormalization,Activation\n",
    "from keras import Model\n",
    "import keras.backend as K\n",
    "import keras.callbacks as kcallbacks\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping\n",
    "def ANN(optimizer = 'sgd',neurons=16,batch_size=1024,epochs=80,activation='relu',patience=8,loss='binary_crossentropy'):\n",
    "    K.clear_session()\n",
    "    inputs=Input(shape=(X.shape[1],))\n",
    "    x=Dense(1000)(inputs)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dropout(0.3)(x)\n",
    "    x=Dense(256)(inputs)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dropout(0.25)(x)\n",
    "    x=Dense(2,activation='softmax')(x)\n",
    "    model=Model(inputs=inputs,outputs=x,name='base_nlp')\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
    "#     model.compile(optimizer=Adam(lr = 0.01),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor=\"loss\", patience = patience)# early stop patience\n",
    "    history = model.fit(X, pd.get_dummies(y).values,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks = [early_stopping],\n",
    "              verbose=0) #verbose set to 1 will show the training process\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.762%\n",
      "Precision: 99.74%\n",
      "Recall: 97.871%\n",
      "F1-score: 98.79599999999999%\n",
      "Time: 220.03672\n",
      "Wall time: 7.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ann = KerasClassifier(build_fn=ANN, verbose=0)\n",
    "ann.fit(X_train,y_train)\n",
    "predictions = ann.predict(X_test)\n",
    "print(\"Accuracy: \"+str(round(accuracy_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Precision: \"+str(round(precision_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Recall: \"+str(round(recall_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"F1-score: \"+str(round(f1_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Time: \"+str(round((t2-t1)/len(y_test)*1000000,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Automated Feature Engineering\n",
    "Feature selection method 1: **Information Gain (IG)**, used to remove irrelevant features to improve model efficiency  \n",
    "Feature selection method 2: **Pearson Correlation**, used to remove redundant features to improve model efficiency and accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant features and select important features\n",
    "def Feature_Importance_IG(data):\n",
    "    features = data.drop(['Label'],axis=1).values  # \"Label\" should be changed to the target class variable name if different\n",
    "    labels = data['Label'].values\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(data.drop(['Label'],axis=1).columns)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    model = lgb.LGBMRegressor(verbose = -1)\n",
    "    model.fit(features, labels)\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': model.feature_importances_})\n",
    "\n",
    "    # Sort features according to importance\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "\n",
    "    # Normalize the feature importances to add up to one\n",
    "    feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n",
    "    feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])\n",
    "    \n",
    "    cumulative_importance=0.90 # Only keep the important features with cumulative importance scores>=90%. It can be changed.\n",
    "\n",
    "    # Make sure most important features are on top\n",
    "    feature_importances = feature_importances.sort_values('cumulative_importance')\n",
    "\n",
    "    # Identify the features not needed to reach the cumulative_importance\n",
    "    record_low_importance = feature_importances[feature_importances['cumulative_importance'] > cumulative_importance]\n",
    "\n",
    "    to_drop = list(record_low_importance['feature'])\n",
    "#     print(feature_importances.drop(['importance'],axis=1))\n",
    "    return to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant features\n",
    "def Feature_Redundancy_Pearson(data):\n",
    "    correlation_threshold=0.90 # Only remove features with the redundancy>90%. It can be changed\n",
    "    features = data.drop(['Label'],axis=1)\n",
    "    corr_matrix = features.corr()\n",
    "\n",
    "    # Extract the upper triangle of the correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "    # Select the features with correlations above the threshold\n",
    "    # Need to use the absolute value\n",
    "    to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "\n",
    "    # Dataframe to hold correlated pairs\n",
    "    record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])\n",
    "\n",
    "    # Iterate through the columns to drop\n",
    "    for column in to_drop:\n",
    "\n",
    "        # Find the correlated features\n",
    "        corr_features = list(upper.index[upper[column].abs() > correlation_threshold])\n",
    "\n",
    "        # Find the correlated values\n",
    "        corr_values = list(upper[column][upper[column].abs() > correlation_threshold])\n",
    "        drop_features = [column for _ in range(len(corr_features))]    \n",
    "\n",
    "        # Record the information (need a temp df for now)\n",
    "        temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n",
    "                                         'corr_feature': corr_features,\n",
    "                                         'corr_value': corr_values})\n",
    "        record_collinear = record_collinear.append(temp_df, ignore_index = True)\n",
    "#     print(record_collinear)\n",
    "    return to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Auto_Feature_Engineering(df):\n",
    "    drop1 = Feature_Importance_IG(df)\n",
    "    dfh1 = df.drop(columns = drop1)\n",
    "    \n",
    "    drop2 = Feature_Redundancy_Pearson(dfh1)\n",
    "    dfh2 = dfh1.drop(columns = drop2)\n",
    "    \n",
    "    return dfh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flow_ID</th>\n",
       "      <th>Src_IP</th>\n",
       "      <th>Src_Port</th>\n",
       "      <th>Dst_IP</th>\n",
       "      <th>Dst_Port</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow_Duration</th>\n",
       "      <th>TotLen_Bwd_Pkts</th>\n",
       "      <th>Flow_Byts/s</th>\n",
       "      <th>Flow_Pkts/s</th>\n",
       "      <th>Flow_IAT_Std</th>\n",
       "      <th>Fwd_IAT_Tot</th>\n",
       "      <th>Bwd_IAT_Std</th>\n",
       "      <th>Bwd_Pkts/s</th>\n",
       "      <th>Pkt_Len_Std</th>\n",
       "      <th>Init_Bwd_Win_Byts</th>\n",
       "      <th>Idle_Mean</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.198248</td>\n",
       "      <td>0.446464</td>\n",
       "      <td>0.821049</td>\n",
       "      <td>0.413136</td>\n",
       "      <td>0.148302</td>\n",
       "      <td>0.874767</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.123680</td>\n",
       "      <td>0.220603</td>\n",
       "      <td>0.006440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028534</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.446413</td>\n",
       "      <td>0.931680</td>\n",
       "      <td>0.222458</td>\n",
       "      <td>0.127585</td>\n",
       "      <td>0.848929</td>\n",
       "      <td>0.466032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.336694</td>\n",
       "      <td>0.458352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.829215</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>0.311194</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.198045</td>\n",
       "      <td>0.446413</td>\n",
       "      <td>0.139234</td>\n",
       "      <td>0.419492</td>\n",
       "      <td>0.867104</td>\n",
       "      <td>0.525838</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.061840</td>\n",
       "      <td>0.374723</td>\n",
       "      <td>0.010947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028534</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.197967</td>\n",
       "      <td>0.446464</td>\n",
       "      <td>0.813747</td>\n",
       "      <td>0.413136</td>\n",
       "      <td>0.148302</td>\n",
       "      <td>0.456471</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498657</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.197967</td>\n",
       "      <td>0.446464</td>\n",
       "      <td>0.813747</td>\n",
       "      <td>0.413136</td>\n",
       "      <td>0.148302</td>\n",
       "      <td>0.442970</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494736</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6247</th>\n",
       "      <td>0.197483</td>\n",
       "      <td>0.446464</td>\n",
       "      <td>0.156291</td>\n",
       "      <td>0.413136</td>\n",
       "      <td>0.148302</td>\n",
       "      <td>0.245577</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.123680</td>\n",
       "      <td>0.390782</td>\n",
       "      <td>0.011417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028534</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6248</th>\n",
       "      <td>0.213862</td>\n",
       "      <td>0.446413</td>\n",
       "      <td>0.928608</td>\n",
       "      <td>0.483051</td>\n",
       "      <td>0.146312</td>\n",
       "      <td>0.759311</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>0.039416</td>\n",
       "      <td>0.049988</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6249</th>\n",
       "      <td>0.119352</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.131007</td>\n",
       "      <td>0.413136</td>\n",
       "      <td>0.009109</td>\n",
       "      <td>0.860102</td>\n",
       "      <td>0.012935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222794</td>\n",
       "      <td>0.025932</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6250</th>\n",
       "      <td>0.978750</td>\n",
       "      <td>0.446516</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.483051</td>\n",
       "      <td>0.164217</td>\n",
       "      <td>0.742551</td>\n",
       "      <td>0.002230</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6251</th>\n",
       "      <td>0.736112</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.006838</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.710894</td>\n",
       "      <td>0.767458</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.064201</td>\n",
       "      <td>0.154344</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003860</td>\n",
       "      <td>0.003719</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6252 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Flow_ID    Src_IP  Src_Port    Dst_IP  Dst_Port  Timestamp  \\\n",
       "0     0.198248  0.446464  0.821049  0.413136  0.148302   0.874767   \n",
       "1     0.000765  0.446413  0.931680  0.222458  0.127585   0.848929   \n",
       "2     0.198045  0.446413  0.139234  0.419492  0.867104   0.525838   \n",
       "3     0.197967  0.446464  0.813747  0.413136  0.148302   0.456471   \n",
       "4     0.197967  0.446464  0.813747  0.413136  0.148302   0.442970   \n",
       "...        ...       ...       ...       ...       ...        ...   \n",
       "6247  0.197483  0.446464  0.156291  0.413136  0.148302   0.245577   \n",
       "6248  0.213862  0.446413  0.928608  0.483051  0.146312   0.759311   \n",
       "6249  0.119352  0.269300  0.131007  0.413136  0.009109   0.860102   \n",
       "6250  0.978750  0.446516  1.000000  0.483051  0.164217   0.742551   \n",
       "6251  0.736112  0.000121  0.006838  0.423729  0.710894   0.767458   \n",
       "\n",
       "      Flow_Duration  TotLen_Bwd_Pkts  Flow_Byts/s  Flow_Pkts/s  Flow_IAT_Std  \\\n",
       "0          0.001241         0.123680     0.220603     0.006440      0.000000   \n",
       "1          0.466032         0.000000     0.000539     0.000023      0.336694   \n",
       "2          0.000726         0.061840     0.374723     0.010947      0.000000   \n",
       "3          0.001493         0.000000     0.000000     0.008042      0.000011   \n",
       "4          0.000827         0.000000     0.000000     0.009627      0.000000   \n",
       "...             ...              ...          ...          ...           ...   \n",
       "6247       0.000696         0.123680     0.390782     0.011417      0.000000   \n",
       "6248       0.000394         0.001426     0.039416     0.049988      0.000116   \n",
       "6249       0.012935         0.000000     0.000000     0.000611      0.000000   \n",
       "6250       0.002230         0.001426     0.005682     0.007195      0.001788   \n",
       "6251       0.001846         0.064201     0.154344     0.004336      0.000000   \n",
       "\n",
       "      Fwd_IAT_Tot  Bwd_IAT_Std  Bwd_Pkts/s  Pkt_Len_Std  Init_Bwd_Win_Byts  \\\n",
       "0        0.000000          0.0    0.016109     0.000000           0.028534   \n",
       "1        0.458352          0.0    0.000023     0.829215           0.001938   \n",
       "2        0.000000          0.0    0.013679     0.000000           0.028534   \n",
       "3        0.000843          0.0    0.006691     0.000000           0.498657   \n",
       "4        0.000000          0.0    0.012028     0.000000           0.494736   \n",
       "...           ...          ...         ...          ...                ...   \n",
       "6247     0.000000          0.0    0.028552     0.000000           0.028534   \n",
       "6248     0.000216          0.0    0.024980     0.000000           0.000000   \n",
       "6249     0.000000          0.0    0.001539     0.000000           0.222794   \n",
       "6250     0.000182          0.0    0.004484     0.000000           0.000000   \n",
       "6251     0.000000          0.0    0.005415     0.000000           0.003860   \n",
       "\n",
       "      Idle_Mean  Label  \n",
       "0      0.002506      1  \n",
       "1      0.311194      1  \n",
       "2      0.001475      1  \n",
       "3      0.001506      1  \n",
       "4      0.001678      1  \n",
       "...         ...    ...  \n",
       "6247   0.001415      1  \n",
       "6248   0.000202      1  \n",
       "6249   0.025932      1  \n",
       "6250   0.001496      1  \n",
       "6251   0.003719      1  \n",
       "\n",
       "[6252 rows x 18 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfh2 = Auto_Feature_Engineering(df)\n",
    "dfh2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split & Balancing (After Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfh2.drop(['Label'],axis=1)\n",
    "y = dfh2['Label']\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2, shuffle=False,random_state = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = Auto_Balancing(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Automated Model Selection\n",
    "Select the best-performing model among five common machine learning models (Naive Bayes, KNN, random forest, LightGBM, and ANN/MLP) by evaluating their learning performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipe = Pipeline([('classifier', GaussianNB())])\n",
    "\n",
    "# Create space of candidate learning algorithms and their hyperparameters\n",
    "search_space = [{'classifier': [GaussianNB()]},\n",
    "                {'classifier': [KNeighborsClassifier()]},\n",
    "                {'classifier': [RandomForestClassifier()]},\n",
    "                {'classifier': [lgb.LGBMClassifier(verbose = -1)]},\n",
    "                {'classifier': [KerasClassifier(build_fn=ANN, verbose=0)]},\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Pipeline(steps=[('classifier', GaussianNB())]),\n",
       "             param_grid=[{'classifier': [GaussianNB()]},\n",
       "                         {'classifier': [KNeighborsClassifier()]},\n",
       "                         {'classifier': [RandomForestClassifier()]},\n",
       "                         {'classifier': [LGBMClassifier(verbose=-1)]},\n",
       "                         {'classifier': [<keras.wrappers.scikit_learn.KerasClassifier object at 0x0000027B91744320>]}])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model:{'classifier': LGBMClassifier(verbose=-1)}\n",
      "Accuracy:0.9993601278976818\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Model:\"+ str(clf.best_params_))\n",
    "print(\"Accuracy:\"+ str(clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([3.19132805e-03, 2.39443779e-03, 3.23404932e-01, 1.25419044e-01,\n",
       "        3.47803822e+00]),\n",
       " 'std_fit_time': array([0.00039907, 0.00048784, 0.01115818, 0.00786784, 0.06721689]),\n",
       " 'mean_score_time': array([0.00179529, 0.14380698, 0.01518211, 0.00498695, 0.07730794]),\n",
       " 'std_score_time': array([0.00039887, 0.00479374, 0.00038913, 0.0006309 , 0.00146038]),\n",
       " 'param_classifier': masked_array(data=[GaussianNB(), KNeighborsClassifier(),\n",
       "                    RandomForestClassifier(), LGBMClassifier(verbose=-1),\n",
       "                    <keras.wrappers.scikit_learn.KerasClassifier object at 0x0000027B91744320>],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': GaussianNB()},\n",
       "  {'classifier': KNeighborsClassifier()},\n",
       "  {'classifier': RandomForestClassifier()},\n",
       "  {'classifier': LGBMClassifier(verbose=-1)},\n",
       "  {'classifier': <keras.wrappers.scikit_learn.KerasClassifier at 0x27b91744320>}],\n",
       " 'split0_test_score': array([0.95123901, 0.99760192, 0.99920064, 1.        ,        nan]),\n",
       " 'split1_test_score': array([0.95683453, 0.99520384, 0.99920064, 0.99920064,        nan]),\n",
       " 'split2_test_score': array([0.9552, 0.9904, 0.9976, 0.9984,    nan]),\n",
       " 'split3_test_score': array([0.9544, 0.9928, 0.9976, 1.    ,    nan]),\n",
       " 'split4_test_score': array([0.9448, 0.9936, 0.9976, 0.9992,    nan]),\n",
       " 'mean_test_score': array([0.95249471, 0.99392115, 0.99824026, 0.99936013,        nan]),\n",
       " 'std_test_score': array([0.00425601, 0.00240632, 0.00078415, 0.00059863,        nan]),\n",
       " 'rank_test_score': array([4, 3, 2, 1, 5])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM model is the best performing machine learning model, and the best cross-validation accuracy is 99.936%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Bayesian Optimization with Tree Parzen Estimator (BO-TPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 10/10 [00:20<00:00,  2.06s/trial, best loss: -1.0]\n",
      "Hyperopt estimated optimum {'classifier_type': 3}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    \n",
    "    classifier_type = params['type']\n",
    "    del params['type']\n",
    "    if classifier_type == 'nb':\n",
    "        clf = GaussianNB()\n",
    "    elif classifier_type == 'knn':\n",
    "        clf = KNeighborsClassifier()\n",
    "    elif classifier_type == 'rf':\n",
    "        clf = RandomForestClassifier()\n",
    "    elif classifier_type == 'lgb':\n",
    "        clf = lgb.LGBMClassifier(verbose = -1)\n",
    "    elif classifier_type == 'ann':\n",
    "        clf = KerasClassifier(build_fn=ANN, verbose=0)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    clf.fit(X_train,y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    score = accuracy_score(y_test,predictions)\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "\n",
    "# Define the hyperparameter configuration space\n",
    "space = hp.choice('classifier_type', [{'type': 'nb'},{'type': 'knn'},{'type': 'rf'},{'type': 'lgb'},{'type': 'ann'},])\n",
    "\n",
    "# Detect the optimal hyperparameter values\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier type 3 is the LightGBM model, and the best hold-out accuracy is 100.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Hyperparameter Optimization\n",
    "Optimize the best performing machine learning model (lightGBM) by tuning its hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 20/20 [00:26<00:00,  1.34s/trial, best loss: -0.99968]\n",
      "LightGBM: Hyperopt estimated optimum {'learning_rate': 0.5636571315681871, 'max_depth': 16.0, 'min_child_samples': 50.0, 'n_estimators': 180.0, 'num_leaves': 1800.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'learning_rate': abs(float(params['learning_rate'])),\n",
    "        \"num_leaves\": int(params['num_leaves']),\n",
    "        \"min_child_samples\": int(params['min_child_samples']),\n",
    "    }\n",
    "    clf = lgb.LGBMClassifier( **params)\n",
    "    score = cross_val_score(clf, X, y, scoring='accuracy', cv=StratifiedKFold(n_splits=5)).mean()\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "\n",
    "# Define the hyperparameter configuration space\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 500, 20),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "    \"learning_rate\":hp.uniform('learning_rate', 0, 1),\n",
    "    \"num_leaves\":hp.quniform('num_leaves',100,2000,100),\n",
    "    \"min_child_samples\":hp.quniform('min_child_samples',10,50,5),\n",
    "}\n",
    "\n",
    "# Detect the optimal hyperparameter values\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=20)\n",
    "print(\"LightGBM: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.968%\n",
      "Precision: 99.983%\n",
      "Recall: 99.983%\n",
      "F1-score: 99.983%\n",
      "Wall time: 2.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = lgb.LGBMClassifier(max_depth=16, learning_rate=  0.5636571315681871, n_estimators = 180, \n",
    "                         num_leaves = 1800, min_child_samples = 50)\n",
    "clf.fit(X,y)\n",
    "scores = cross_val_score(clf, X, y, cv=5,scoring='accuracy')\n",
    "print(\"Accuracy: \"+ str(round(scores.mean(),5)*100)+\"%\")\n",
    "scores = cross_val_score(clf, X, y, cv=5,scoring='precision')\n",
    "print(\"Precision: \"+ str(round(scores.mean(),5)*100)+\"%\")\n",
    "scores = cross_val_score(clf, X, y, cv=5,scoring='recall')\n",
    "print(\"Recall: \"+ str(round(scores.mean(),5)*100)+\"%\")\n",
    "scores = cross_val_score(clf, X, y, cv=5,scoring='f1')\n",
    "print(\"F1-score: \"+ str(round(scores.mean(),5)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter optimization, the cross-validation accuracy has been improved from 99.936%% to 99.968%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold-out validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 50/50 [00:15<00:00,  3.29trial/s, best loss: -1.0]\n",
      "LightGBM: Hyperopt estimated optimum {'learning_rate': 0.17566405992887468, 'max_depth': 45.0, 'min_child_samples': 45.0, 'n_estimators': 300.0, 'num_leaves': 400.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'learning_rate': abs(float(params['learning_rate'])),\n",
    "        \"num_leaves\": int(params['num_leaves']),\n",
    "        \"min_child_samples\": int(params['min_child_samples']),\n",
    "    }\n",
    "    clf = lgb.LGBMClassifier( **params)\n",
    "    clf.fit(X_train,y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    score = accuracy_score(y_test,predictions)\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "\n",
    "# Define the hyperparameter configuration space\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 500, 20),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "    \"learning_rate\":hp.uniform('learning_rate', 0, 1),\n",
    "    \"num_leaves\":hp.quniform('num_leaves',100,2000,100),\n",
    "    \"min_child_samples\":hp.quniform('min_child_samples',10,50,5),\n",
    "}\n",
    "\n",
    "# Detect the optimal hyperparameter values\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50)\n",
    "print(\"LightGBM: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0%\n",
      "Precision: 100.0%\n",
      "Recall: 100.0%\n",
      "F1-score: 100.0%\n",
      "Wall time: 355 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = lgb.LGBMClassifier(max_depth=45, learning_rate= 0.17566405992887468, n_estimators = 300, \n",
    "                         num_leaves = 400, min_child_samples = 45)\n",
    "clf.fit(X_train,y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(\"Accuracy: \"+str(round(accuracy_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Precision: \"+str(round(precision_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Recall: \"+str(round(recall_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"F1-score: \"+str(round(f1_score(y_test,predictions),5)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After hyperparameter optimization, the hold-out accuracy has been improved from 100.0% to 100.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Combined Algorithm Selection and Hyperparameter tuning (CASH)\n",
    "CASH is the process of combining the two AutoML procedures: model selection and hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method: Particle Swarm Optimization (PSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'lightgbm', 'n_neighbors': None, 'learning_rate': 0.88427734375, 'max_depth': 28.44482421875, 'min_child_samples': 39.66796875, 'n_estimators': 78.9453125, 'num_leaves': 251.220703125}\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import optunity\n",
    "import optunity.metrics\n",
    "\n",
    "search = {'algorithm': {'k-nn': {'n_neighbors': [3, 10]},\n",
    "                        'naive-bayes': None,\n",
    "                        'random-forest': {\n",
    "                                'n_estimators': [20, 100],\n",
    "                                'max_features': [5, 12],\n",
    "                                'max_depth': [5,50],\n",
    "                                \"min_samples_split\":[2,11],\n",
    "                                \"min_samples_leaf\":[1,11]},\n",
    "                        'lightgbm': {\n",
    "                                'n_estimators': [20, 100],\n",
    "                                'max_depth': [5, 50],\n",
    "                                'learning_rate': (0, 1),\n",
    "                                \"num_leaves\":[100, 2000],\n",
    "                                \"min_child_samples\":[10, 50],\n",
    "                                    },\n",
    "                        'ann': {\n",
    "                                'neurons': [10, 100],\n",
    "                                'epochs': [20, 50],\n",
    "                                'patience': [3, 20],\n",
    "                                }\n",
    "                        }\n",
    "          \n",
    "         }\n",
    "def performance(\n",
    "                algorithm, n_neighbors=None, \n",
    "    n_estimators=None, max_features=None,max_depth=None,min_samples_split=None,min_samples_leaf=None,\n",
    "    learning_rate=None,num_leaves=None,min_child_samples=None,\n",
    "    neurons=None,epochs=None,patience=None\n",
    "):\n",
    "    # fit the model\n",
    "    if algorithm == 'k-nn':\n",
    "        model = KNeighborsClassifier(n_neighbors=int(n_neighbors))\n",
    "    elif algorithm == 'naive-bayes':\n",
    "        model = GaussianNB()\n",
    "    elif algorithm == 'random-forest':\n",
    "        model = RandomForestClassifier(n_estimators=int(n_estimators),\n",
    "                                       max_features=int(max_features),\n",
    "                                       max_depth=int(max_depth),\n",
    "                                       min_samples_split=int(min_samples_split),\n",
    "                                       min_samples_leaf=int(min_samples_leaf))\n",
    "    elif algorithm == 'lightgbm':\n",
    "        model = lgb.LGBMClassifier(n_estimators=int(n_estimators),\n",
    "                                   max_depth=int(max_depth),\n",
    "                                   learning_rate=float(learning_rate),\n",
    "                                   num_leaves=int(num_leaves),\n",
    "                                   min_child_samples=int(min_child_samples),\n",
    "                                  )\n",
    "    elif algorithm == 'ann':\n",
    "        model = KerasClassifier(build_fn=ANN, verbose=0,\n",
    "                               neurons=int(neurons),\n",
    "                                epochs=int(epochs),\n",
    "                                patience=int(patience)\n",
    "                               )\n",
    "    else:\n",
    "        raise ArgumentError('Unknown algorithm: %s' % algorithm)\n",
    "# predict the test set\n",
    "    model.fit(X_train,y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    score = accuracy_score(y_test,prediction)\n",
    "    return score\n",
    "\n",
    "# Run the CASH process\n",
    "optimal_configuration, info, _ = optunity.maximize_structured(performance, \n",
    "                                                              search_space=search, \n",
    "                                                              num_evals=50)\n",
    "print(optimal_configuration)\n",
    "print(info.optimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0%\n",
      "Precision: 100.0%\n",
      "Recall: 100.0%\n",
      "F1-score: 100.0%\n",
      "Wall time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = lgb.LGBMClassifier(max_depth=28, learning_rate= 0.88427734375, n_estimators = 78, \n",
    "                         num_leaves = 251, min_child_samples = 40)\n",
    "clf.fit(X_train,y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(\"Accuracy: \"+str(round(accuracy_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Precision: \"+str(round(precision_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Recall: \"+str(round(recall_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"F1-score: \"+str(round(f1_score(y_test,predictions),5)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM with the above hyperparameter values is identified as the optimal model"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
